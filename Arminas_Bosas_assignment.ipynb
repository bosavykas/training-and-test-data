{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "### Practical Session\n",
    "\n",
    "<br/> Student Arminas Bosas\n",
    "<br/> email: arminas.bo9671@go.kauko.lt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. Data loading and pre-processing\n",
    "2. Building the RNN\n",
    "3. Train and deploy the RNN\n",
    "4. Improving the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.aerotime.aero/aviation-blog/wp-content/uploads/2016/06/Boeing-Logo.svg.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a 10-year history of the Boeing Stock prices predict the stock values for the period of the recent most month that are not included in the historical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data-sets\n",
    "\n",
    "The data-sets are two comma-separated values files (CSV) and contain a data table of 2537 records for training and a table of 23 records for testing.\n",
    "\n",
    "They can be found at the [Yahoo.com website](https://finance.yahoo.com/quote/BA?p=BA&.tsrc=fin-srch)\n",
    "\n",
    "Known alternative location: Github user [bosavykas](https://github.com/bosavykas/training-and-test-data) \n",
    "\n",
    "Open a terminal and use the wget command to get it of the selected location. Example:\n",
    "\n",
    "```shell\n",
    "wget https://raw.githubusercontent.com/bosavykas/training-and-test-data/main/training_data.csv\n",
    "\n",
    "wget https://raw.githubusercontent.com/bosavykas/training-and-test-data/main/test_data.csv \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries\n",
    "\n",
    "We need 3 main libraries:\n",
    "\n",
    "- [Numpy](http://www.numpy.org): it is the fundamental package for scientific computing with Python. It contains among other things a powerful N-dimensional array object that can be used as an efficient multi-dimensional container of generic data. Arbitrary data-types can be defined.\n",
    "- [matplotlib](https://matplotlib.org):  it is a Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms.\n",
    "- [pandas](https://pandas.pydata.org): is a software library written for the Python programming language for data manipulation and analysis. In particular, it offers data structures and operations for manipulating numerical tables and time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library importation\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset description: the open high, low and close values of the Boeing Stock from 2009 to 2019.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset importation\n",
    "\n",
    "# loading contents of the file for July 1st 2009 - July 31st 2019 \n",
    "dataset_train = pd.read_csv('training_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-07-01</td>\n",
       "      <td>42.630001</td>\n",
       "      <td>42.950001</td>\n",
       "      <td>42.150002</td>\n",
       "      <td>42.230000</td>\n",
       "      <td>32.061939</td>\n",
       "      <td>6346700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-07-02</td>\n",
       "      <td>41.810001</td>\n",
       "      <td>41.810001</td>\n",
       "      <td>40.619999</td>\n",
       "      <td>40.830002</td>\n",
       "      <td>30.999023</td>\n",
       "      <td>7201500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-07-06</td>\n",
       "      <td>40.380001</td>\n",
       "      <td>40.759998</td>\n",
       "      <td>39.919998</td>\n",
       "      <td>40.560001</td>\n",
       "      <td>30.794052</td>\n",
       "      <td>7132800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-07-07</td>\n",
       "      <td>40.500000</td>\n",
       "      <td>40.689999</td>\n",
       "      <td>38.939999</td>\n",
       "      <td>39.040001</td>\n",
       "      <td>29.640028</td>\n",
       "      <td>7701600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-07-08</td>\n",
       "      <td>39.119999</td>\n",
       "      <td>39.830002</td>\n",
       "      <td>38.919998</td>\n",
       "      <td>39.549999</td>\n",
       "      <td>30.027231</td>\n",
       "      <td>8800400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date       Open       High        Low      Close  Adj Close   Volume\n",
       "0  2009-07-01  42.630001  42.950001  42.150002  42.230000  32.061939  6346700\n",
       "1  2009-07-02  41.810001  41.810001  40.619999  40.830002  30.999023  7201500\n",
       "2  2009-07-06  40.380001  40.759998  39.919998  40.560001  30.794052  7132800\n",
       "3  2009-07-07  40.500000  40.689999  38.939999  39.040001  29.640028  7701600\n",
       "4  2009-07-08  39.119999  39.830002  38.919998  39.549999  30.027231  8800400"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a subtable of related openings\n",
    "# The .values option causes this vector to form a numpy array\n",
    "training_set = dataset_train.iloc[:, 1:2].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 42.630001],\n",
       "       [ 41.810001],\n",
       "       [ 40.380001],\n",
       "       ...,\n",
       "       [346.200012],\n",
       "       [344.98999 ],\n",
       "       [339.839996]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy arrays do not support the view() and head() methods. [More on accessing the numpy data](https://jakevdp.github.io/PythonDataScienceHandbook/02.02-the-basics-of-numpy-arrays.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.part.lt/img/33d457ecbc861af3ae2f7cf0da2e93ef530.PNG\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "Next we need to rescale our data to the range from 0 to 1. \n",
    "\n",
    "Feature scaling is essential as discussed if the Features lecture and needs to be applied to both the training and test sets.\n",
    "\n",
    "It is computed using the ScikitLearn library [MinMaxScaler()](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler) which transforms the selected feature by scaling it to a given range. If more than one, this estimator scales and translates each feature individually such that it is in the given range on the training set, i.e. between zero and one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the feature\n",
    "\n",
    "# importing the MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a scaler instance to rescale all data to the range of 0.0 to 1.0 \n",
    "sc = MinMaxScaler(feature_range = (0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a certain training set of scaled values\n",
    "training_set_scaled = sc.fit_transform(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00862641],\n",
       "       [0.00661113],\n",
       "       [0.00309666],\n",
       "       ...,\n",
       "       [0.7547003 ],\n",
       "       [0.75172647],\n",
       "       [0.7390695 ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the training set to dependent and independent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?id=1bckuLGZCeLUzNA-xJCGOODzC-4n2U-If\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data structure with 90 timesteps and 1 output\n",
    "\n",
    "# The 90 stock prices in the last 3 months before today\n",
    "X_train = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2537, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Today's stock price\n",
    "y_train = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting from day 90, then going back 90 days \n",
    "for i in range(90, 2537): \n",
    "    # Number 0 defines column ID, which is the only column in this case    \n",
    "    # Putting the last 90 days values in one row of X_train\n",
    "    X_train.append(training_set_scaled[i-90:i, 0]) \n",
    "    y_train.append(training_set_scaled[i, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = np.array(X_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00862641, 0.00661113, 0.00309666, ..., 0.02224188, 0.02251223,\n",
       "        0.02302834],\n",
       "       [0.00661113, 0.00309666, 0.00339158, ..., 0.02251223, 0.02302834,\n",
       "        0.02602669],\n",
       "       [0.00309666, 0.00339158, 0.        , ..., 0.02302834, 0.02602669,\n",
       "        0.02676399],\n",
       "       ...,\n",
       "       [0.81599451, 0.8247438 , 0.82312173, ..., 0.82368694, 0.81388088,\n",
       "        0.77922777],\n",
       "       [0.8247438 , 0.82312173, 0.81021898, ..., 0.81388088, 0.77922777,\n",
       "        0.7547003 ],\n",
       "       [0.82312173, 0.81021898, 0.8009044 , ..., 0.77922777, 0.7547003 ,\n",
       "        0.75172647]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping the Matrix\n",
    "\n",
    "We need to add a new matrix dimension to accommodate the indicator (predictor). \n",
    "\n",
    "NumPy matrices are tensors (3D) and essentially we need to specify that our matrix consists of **90 days** (dimension x) times **total days in data set** (dimension y) times **1 value per matrix cell (scalar)** (dimension z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.pixabay.com/photo/2015/03/22/17/34/cubic-684961_1280.jpg\" width=\"500\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to add the stock value of somebody else together with the the past 90 days of Boeing, we need to change the length of the 3 dimension to  2.  RNN training tables are 3D!!! Read: [Reshaping NumPy Array | Numpy Array Reshape Examples](https://backtobazics.com/python/python-reshaping-numpy-array-examples/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformating the data matrix, we retain the 2 original dimensions and adding a third one with the depth of 1\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN initialization\n",
    "\n",
    "- Import the sequential model from the Keras API;\n",
    "- Import the Dense layer template from the Keras API;\n",
    "- Import the LSTM model from the Keras API\n",
    "- Create an instance of the sequential model called regressor because we want to predict a continuous value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the RNN as an arrangement of layers\n",
    "regressor = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add First Layer\n",
    "\n",
    "We first add an object of the LSTM class! \n",
    "\n",
    "- The first argument is the number of units or LSTM memory cells. Include many neurons to address the high dimensionality of the problem; say 50 neurons! \n",
    "- Second arg: return sequences = true; stacked LSTM !\n",
    "- Third arg: input 3D shape: observations vs time steps vs number of indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the input layer and the LSTM layer\n",
    "regressor.add(LSTM(units = 50, return_sequences = True, input_shape =  (X_train.shape[1], 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The argument is the dropout rate to ignore in the layers (20%) \n",
    "# For example 100 units * 20% = 20 units will be dropped each time\n",
    "regressor.add(Dropout(0.2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add More Layers\n",
    "\n",
    "We can add more LSTM layers but along with Dropout regularization to make sure we avoid overfitting! \n",
    "\n",
    "We don’t need to add the shape of the layer again because it is recognized automatically from the number of input units.\n",
    "\n",
    "The last layer does not return a sequence but connected directly to a fully connected output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a second LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a third LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a fourth LSTM layer and some Dropout regularisation\n",
    "# I removed the return_sequences message, because we no longer return an arrangement, but a value instead\n",
    "regressor.add(LSTM(units = 50))\n",
    "regressor.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Output Layer & Compile\n",
    "\n",
    "The output has 1 dimension , i.e. one value to be predicted thus or output fully connected layer has dimensionality = 1.\n",
    "\n",
    "- **Optimizer**: rmsprop is recommended in the Keras documentation. The Adam optimizer is also a powerful choice.\n",
    "- **Loss function**: regression problems take the mean square error as most common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the output layer\n",
    "regressor.add(Dense(units = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the RNN\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and deploy the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the RNN to the Training set\n",
    "\n",
    "We now want to train our RNN using the data in our **Training Set X** and **predictors in y** (ground truth in this case). Parameters that can be specified are the:\n",
    "\n",
    "- **Batch size**:  update the cell weights not on every stock price on every batch_size values; \n",
    "- **Number of epochs**: how many iterations to be used, i.e. number of forward and backward propagations for the update of the weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "77/77 [==============================] - 11s 144ms/step - loss: 0.0088\n",
      "Epoch 2/100\n",
      "77/77 [==============================] - 11s 139ms/step - loss: 0.0023\n",
      "Epoch 3/100\n",
      "77/77 [==============================] - 11s 142ms/step - loss: 0.0020\n",
      "Epoch 4/100\n",
      "77/77 [==============================] - 11s 140ms/step - loss: 0.0018\n",
      "Epoch 5/100\n",
      "77/77 [==============================] - 11s 143ms/step - loss: 0.0017\n",
      "Epoch 6/100\n",
      "77/77 [==============================] - 12s 155ms/step - loss: 0.0016\n",
      "Epoch 7/100\n",
      "77/77 [==============================] - 11s 145ms/step - loss: 0.0017\n",
      "Epoch 8/100\n",
      "77/77 [==============================] - 10s 136ms/step - loss: 0.0017\n",
      "Epoch 9/100\n",
      "77/77 [==============================] - 10s 136ms/step - loss: 0.0016\n",
      "Epoch 10/100\n",
      "77/77 [==============================] - 11s 137ms/step - loss: 0.0014\n",
      "Epoch 11/100\n",
      "77/77 [==============================] - 11s 136ms/step - loss: 0.0013\n",
      "Epoch 12/100\n",
      "77/77 [==============================] - 10s 135ms/step - loss: 0.0018\n",
      "Epoch 13/100\n",
      "77/77 [==============================] - 11s 140ms/step - loss: 0.0015\n",
      "Epoch 14/100\n",
      "77/77 [==============================] - 10s 135ms/step - loss: 0.0013\n",
      "Epoch 15/100\n",
      "77/77 [==============================] - 10s 135ms/step - loss: 0.0015\n",
      "Epoch 16/100\n",
      "77/77 [==============================] - 11s 137ms/step - loss: 0.0012\n",
      "Epoch 17/100\n",
      "77/77 [==============================] - 10s 136ms/step - loss: 0.0012\n",
      "Epoch 18/100\n",
      "77/77 [==============================] - 11s 143ms/step - loss: 0.0013\n",
      "Epoch 19/100\n",
      "77/77 [==============================] - 11s 139ms/step - loss: 0.0012\n",
      "Epoch 20/100\n",
      "77/77 [==============================] - 10s 136ms/step - loss: 0.0014\n",
      "Epoch 21/100\n",
      "77/77 [==============================] - 10s 136ms/step - loss: 0.0010\n",
      "Epoch 22/100\n",
      "77/77 [==============================] - 10s 136ms/step - loss: 0.0011\n",
      "Epoch 23/100\n",
      "77/77 [==============================] - 11s 136ms/step - loss: 0.0010\n",
      "Epoch 24/100\n",
      "77/77 [==============================] - 11s 138ms/step - loss: 0.0010\n",
      "Epoch 25/100\n",
      "77/77 [==============================] - 11s 137ms/step - loss: 0.0011\n",
      "Epoch 26/100\n",
      "77/77 [==============================] - 11s 139ms/step - loss: 0.0010\n",
      "Epoch 27/100\n",
      "77/77 [==============================] - 11s 137ms/step - loss: 9.8220e-04\n",
      "Epoch 28/100\n",
      "77/77 [==============================] - 11s 137ms/step - loss: 0.0012\n",
      "Epoch 29/100\n",
      "77/77 [==============================] - 11s 137ms/step - loss: 0.0010\n",
      "Epoch 30/100\n",
      "77/77 [==============================] - 11s 137ms/step - loss: 8.2696e-04\n",
      "Epoch 31/100\n",
      "77/77 [==============================] - 11s 139ms/step - loss: 8.8932e-04\n",
      "Epoch 32/100\n",
      "77/77 [==============================] - 11s 138ms/step - loss: 9.2244e-04\n",
      "Epoch 33/100\n",
      "77/77 [==============================] - 11s 138ms/step - loss: 8.1759e-04\n",
      "Epoch 34/100\n",
      "77/77 [==============================] - 11s 138ms/step - loss: 8.5384e-04\n",
      "Epoch 35/100\n",
      "77/77 [==============================] - 11s 138ms/step - loss: 8.7938e-04\n",
      "Epoch 36/100\n",
      "77/77 [==============================] - 11s 137ms/step - loss: 8.2169e-04\n",
      "Epoch 37/100\n",
      "77/77 [==============================] - 11s 139ms/step - loss: 0.0011\n",
      "Epoch 38/100\n",
      "77/77 [==============================] - 11s 137ms/step - loss: 9.2750e-04\n",
      "Epoch 39/100\n",
      "77/77 [==============================] - 11s 138ms/step - loss: 0.0010\n",
      "Epoch 40/100\n",
      "77/77 [==============================] - 10s 136ms/step - loss: 8.4481e-04\n",
      "Epoch 41/100\n",
      "77/77 [==============================] - 11s 137ms/step - loss: 7.9189e-04\n",
      "Epoch 42/100\n",
      "77/77 [==============================] - 11s 138ms/step - loss: 7.8931e-04\n",
      "Epoch 43/100\n",
      "77/77 [==============================] - 11s 137ms/step - loss: 7.6082e-04\n",
      "Epoch 44/100\n",
      "77/77 [==============================] - 11s 138ms/step - loss: 8.2328e-04\n",
      "Epoch 45/100\n",
      "77/77 [==============================] - 11s 138ms/step - loss: 8.6717e-04\n",
      "Epoch 46/100\n",
      "77/77 [==============================] - 11s 137ms/step - loss: 7.4737e-04\n",
      "Epoch 47/100\n",
      "77/77 [==============================] - 11s 138ms/step - loss: 7.8238e-04\n",
      "Epoch 48/100\n",
      "77/77 [==============================] - 11s 138ms/step - loss: 8.0013e-04\n",
      "Epoch 49/100\n",
      "77/77 [==============================] - 11s 138ms/step - loss: 6.8059e-04\n",
      "Epoch 50/100\n",
      "77/77 [==============================] - 11s 137ms/step - loss: 7.0924e-04\n",
      "Epoch 51/100\n",
      "77/77 [==============================] - 11s 137ms/step - loss: 7.1670e-04\n",
      "Epoch 52/100\n",
      "77/77 [==============================] - 11s 137ms/step - loss: 7.4172e-04\n",
      "Epoch 53/100\n",
      "77/77 [==============================] - 11s 137ms/step - loss: 8.7733e-04\n",
      "Epoch 54/100\n",
      "77/77 [==============================] - 11s 137ms/step - loss: 6.9365e-04\n",
      "Epoch 55/100\n",
      "77/77 [==============================] - 11s 137ms/step - loss: 7.5950e-04\n",
      "Epoch 56/100\n",
      "77/77 [==============================] - 11s 137ms/step - loss: 7.1808e-04\n",
      "Epoch 57/100\n",
      "77/77 [==============================] - 11s 137ms/step - loss: 7.1981e-04\n",
      "Epoch 58/100\n",
      "77/77 [==============================] - 11s 137ms/step - loss: 7.6571e-04\n",
      "Epoch 59/100\n",
      "77/77 [==============================] - 11s 138ms/step - loss: 8.0126e-04\n",
      "Epoch 60/100\n",
      "77/77 [==============================] - 11s 138ms/step - loss: 6.0389e-04\n",
      "Epoch 61/100\n",
      "77/77 [==============================] - 11s 138ms/step - loss: 7.0660e-04\n",
      "Epoch 62/100\n",
      "77/77 [==============================] - 11s 137ms/step - loss: 7.3742e-04\n",
      "Epoch 63/100\n",
      "77/77 [==============================] - 11s 138ms/step - loss: 7.9529e-04\n",
      "Epoch 64/100\n",
      "77/77 [==============================] - 11s 137ms/step - loss: 7.8774e-04\n",
      "Epoch 65/100\n",
      "77/77 [==============================] - 11s 137ms/step - loss: 8.0471e-04\n",
      "Epoch 66/100\n",
      "77/77 [==============================] - 11s 137ms/step - loss: 6.9698e-04\n",
      "Epoch 67/100\n",
      "77/77 [==============================] - 11s 138ms/step - loss: 6.8039e-04\n",
      "Epoch 68/100\n",
      "77/77 [==============================] - 11s 137ms/step - loss: 6.5501e-04\n",
      "Epoch 69/100\n",
      "77/77 [==============================] - 11s 138ms/step - loss: 6.6222e-04\n",
      "Epoch 70/100\n",
      "77/77 [==============================] - 11s 137ms/step - loss: 7.4220e-04\n",
      "Epoch 71/100\n",
      "77/77 [==============================] - 11s 137ms/step - loss: 7.6333e-04\n",
      "Epoch 72/100\n",
      "77/77 [==============================] - 11s 139ms/step - loss: 6.6594e-04\n",
      "Epoch 73/100\n",
      "77/77 [==============================] - 11s 137ms/step - loss: 6.4626e-04\n",
      "Epoch 74/100\n",
      "77/77 [==============================] - 11s 137ms/step - loss: 7.0823e-04\n",
      "Epoch 75/100\n",
      "77/77 [==============================] - 11s 137ms/step - loss: 6.9659e-04\n",
      "Epoch 76/100\n",
      "77/77 [==============================] - 11s 138ms/step - loss: 8.1723e-04\n",
      "Epoch 77/100\n",
      "77/77 [==============================] - 11s 137ms/step - loss: 7.0793e-04\n",
      "Epoch 78/100\n",
      "77/77 [==============================] - 11s 137ms/step - loss: 6.4975e-04\n",
      "Epoch 79/100\n",
      "77/77 [==============================] - 11s 138ms/step - loss: 6.3058e-04\n",
      "Epoch 80/100\n",
      "77/77 [==============================] - 11s 142ms/step - loss: 6.5314e-04\n",
      "Epoch 81/100\n",
      "77/77 [==============================] - 11s 138ms/step - loss: 6.0993e-04\n",
      "Epoch 82/100\n",
      "77/77 [==============================] - 11s 139ms/step - loss: 6.8592e-04\n",
      "Epoch 83/100\n",
      "77/77 [==============================] - 11s 138ms/step - loss: 6.5400e-04\n",
      "Epoch 84/100\n",
      "77/77 [==============================] - 11s 138ms/step - loss: 6.7163e-04\n",
      "Epoch 85/100\n",
      "77/77 [==============================] - 11s 137ms/step - loss: 7.0998e-04\n",
      "Epoch 86/100\n",
      "77/77 [==============================] - 11s 138ms/step - loss: 6.0640e-04\n",
      "Epoch 87/100\n",
      "77/77 [==============================] - 11s 139ms/step - loss: 6.1768e-04\n",
      "Epoch 88/100\n",
      "77/77 [==============================] - 11s 138ms/step - loss: 6.8109e-04\n",
      "Epoch 89/100\n",
      "77/77 [==============================] - 11s 139ms/step - loss: 5.6913e-04\n",
      "Epoch 90/100\n",
      "77/77 [==============================] - 11s 138ms/step - loss: 6.2979e-04\n",
      "Epoch 91/100\n",
      "77/77 [==============================] - 11s 139ms/step - loss: 6.4391e-04\n",
      "Epoch 92/100\n",
      "77/77 [==============================] - 11s 139ms/step - loss: 6.9729e-04\n",
      "Epoch 93/100\n",
      "77/77 [==============================] - 11s 139ms/step - loss: 5.7395e-04\n",
      "Epoch 94/100\n",
      "77/77 [==============================] - 11s 137ms/step - loss: 5.7767e-04\n",
      "Epoch 95/100\n",
      "77/77 [==============================] - 11s 137ms/step - loss: 6.0636e-04\n",
      "Epoch 96/100\n",
      "77/77 [==============================] - 11s 138ms/step - loss: 6.5257e-04\n",
      "Epoch 97/100\n",
      "77/77 [==============================] - 11s 137ms/step - loss: 6.1164e-04\n",
      "Epoch 98/100\n",
      "77/77 [==============================] - 11s 137ms/step - loss: 6.0991e-04\n",
      "Epoch 99/100\n",
      "77/77 [==============================] - 11s 141ms/step - loss: 6.2882e-04\n",
      "Epoch 100/100\n",
      "77/77 [==============================] - 11s 137ms/step - loss: 6.0019e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f13e4161bb0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the RNN to the Training set\n",
    "regressor.fit(X_train, y_train, epochs = 100, batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Predictions\n",
    "\n",
    "Create a data-frame by importing the Boeing Stock Price Test set for August of 2019 using pandas and make it a numpy array.\n",
    "\n",
    "There are 21 financial days in one month, weekends are excluded!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-08-01</td>\n",
       "      <td>341.910004</td>\n",
       "      <td>344.290009</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>334.290009</td>\n",
       "      <td>328.335938</td>\n",
       "      <td>6465300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-08-02</td>\n",
       "      <td>334.559998</td>\n",
       "      <td>340.540009</td>\n",
       "      <td>332.320007</td>\n",
       "      <td>339.559998</td>\n",
       "      <td>333.512085</td>\n",
       "      <td>4675400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-08-05</td>\n",
       "      <td>332.459991</td>\n",
       "      <td>333.369995</td>\n",
       "      <td>326.959991</td>\n",
       "      <td>331.059998</td>\n",
       "      <td>325.163452</td>\n",
       "      <td>5876100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-08-06</td>\n",
       "      <td>333.989990</td>\n",
       "      <td>335.700012</td>\n",
       "      <td>329.140015</td>\n",
       "      <td>332.450012</td>\n",
       "      <td>326.528748</td>\n",
       "      <td>3653300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-08-07</td>\n",
       "      <td>328.600006</td>\n",
       "      <td>333.649994</td>\n",
       "      <td>324.570007</td>\n",
       "      <td>331.380005</td>\n",
       "      <td>325.477783</td>\n",
       "      <td>4187600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-08-08</td>\n",
       "      <td>331.299988</td>\n",
       "      <td>336.429993</td>\n",
       "      <td>328.859985</td>\n",
       "      <td>336.350006</td>\n",
       "      <td>332.420715</td>\n",
       "      <td>3673000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2019-08-09</td>\n",
       "      <td>334.059998</td>\n",
       "      <td>339.079987</td>\n",
       "      <td>333.170013</td>\n",
       "      <td>337.549988</td>\n",
       "      <td>333.606689</td>\n",
       "      <td>3102900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2019-08-12</td>\n",
       "      <td>333.679993</td>\n",
       "      <td>336.010010</td>\n",
       "      <td>331.369995</td>\n",
       "      <td>332.940002</td>\n",
       "      <td>329.050537</td>\n",
       "      <td>2542500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2019-08-13</td>\n",
       "      <td>331.820007</td>\n",
       "      <td>337.399994</td>\n",
       "      <td>330.500000</td>\n",
       "      <td>332.859985</td>\n",
       "      <td>328.971466</td>\n",
       "      <td>3689700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2019-08-14</td>\n",
       "      <td>328.350006</td>\n",
       "      <td>329.540009</td>\n",
       "      <td>320.410004</td>\n",
       "      <td>320.420013</td>\n",
       "      <td>316.676788</td>\n",
       "      <td>5110400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2019-08-15</td>\n",
       "      <td>323.000000</td>\n",
       "      <td>328.070007</td>\n",
       "      <td>319.549988</td>\n",
       "      <td>328.000000</td>\n",
       "      <td>324.168243</td>\n",
       "      <td>4721800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2019-08-16</td>\n",
       "      <td>330.220001</td>\n",
       "      <td>331.160004</td>\n",
       "      <td>326.480011</td>\n",
       "      <td>330.450012</td>\n",
       "      <td>326.589661</td>\n",
       "      <td>3866600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2019-08-19</td>\n",
       "      <td>334.179993</td>\n",
       "      <td>335.910004</td>\n",
       "      <td>331.399994</td>\n",
       "      <td>333.779999</td>\n",
       "      <td>329.880707</td>\n",
       "      <td>2335600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2019-08-20</td>\n",
       "      <td>332.470001</td>\n",
       "      <td>333.500000</td>\n",
       "      <td>328.880005</td>\n",
       "      <td>331.750000</td>\n",
       "      <td>327.874451</td>\n",
       "      <td>2039000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2019-08-21</td>\n",
       "      <td>335.480011</td>\n",
       "      <td>342.579987</td>\n",
       "      <td>333.209991</td>\n",
       "      <td>339.989990</td>\n",
       "      <td>336.018188</td>\n",
       "      <td>3265600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2019-08-22</td>\n",
       "      <td>342.429993</td>\n",
       "      <td>356.440002</td>\n",
       "      <td>341.010010</td>\n",
       "      <td>354.410004</td>\n",
       "      <td>350.269714</td>\n",
       "      <td>8320900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2019-08-23</td>\n",
       "      <td>355.350006</td>\n",
       "      <td>369.690002</td>\n",
       "      <td>354.350006</td>\n",
       "      <td>356.010010</td>\n",
       "      <td>351.851044</td>\n",
       "      <td>11518700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2019-08-26</td>\n",
       "      <td>365.040009</td>\n",
       "      <td>367.339996</td>\n",
       "      <td>356.890015</td>\n",
       "      <td>359.040009</td>\n",
       "      <td>354.845642</td>\n",
       "      <td>4661100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2019-08-27</td>\n",
       "      <td>359.420013</td>\n",
       "      <td>361.309998</td>\n",
       "      <td>353.619995</td>\n",
       "      <td>354.730011</td>\n",
       "      <td>350.585999</td>\n",
       "      <td>3740300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2019-08-28</td>\n",
       "      <td>353.309998</td>\n",
       "      <td>360.190002</td>\n",
       "      <td>351.179993</td>\n",
       "      <td>359.970001</td>\n",
       "      <td>355.764771</td>\n",
       "      <td>2402000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2019-08-29</td>\n",
       "      <td>363.850006</td>\n",
       "      <td>366.040009</td>\n",
       "      <td>360.170013</td>\n",
       "      <td>362.739990</td>\n",
       "      <td>358.502411</td>\n",
       "      <td>2717500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2019-08-30</td>\n",
       "      <td>364.369995</td>\n",
       "      <td>366.369995</td>\n",
       "      <td>362.399994</td>\n",
       "      <td>364.089996</td>\n",
       "      <td>359.836639</td>\n",
       "      <td>3147000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date        Open        High         Low       Close   Adj Close  \\\n",
       "0   2019-08-01  341.910004  344.290009  333.000000  334.290009  328.335938   \n",
       "1   2019-08-02  334.559998  340.540009  332.320007  339.559998  333.512085   \n",
       "2   2019-08-05  332.459991  333.369995  326.959991  331.059998  325.163452   \n",
       "3   2019-08-06  333.989990  335.700012  329.140015  332.450012  326.528748   \n",
       "4   2019-08-07  328.600006  333.649994  324.570007  331.380005  325.477783   \n",
       "5   2019-08-08  331.299988  336.429993  328.859985  336.350006  332.420715   \n",
       "6   2019-08-09  334.059998  339.079987  333.170013  337.549988  333.606689   \n",
       "7   2019-08-12  333.679993  336.010010  331.369995  332.940002  329.050537   \n",
       "8   2019-08-13  331.820007  337.399994  330.500000  332.859985  328.971466   \n",
       "9   2019-08-14  328.350006  329.540009  320.410004  320.420013  316.676788   \n",
       "10  2019-08-15  323.000000  328.070007  319.549988  328.000000  324.168243   \n",
       "11  2019-08-16  330.220001  331.160004  326.480011  330.450012  326.589661   \n",
       "12  2019-08-19  334.179993  335.910004  331.399994  333.779999  329.880707   \n",
       "13  2019-08-20  332.470001  333.500000  328.880005  331.750000  327.874451   \n",
       "14  2019-08-21  335.480011  342.579987  333.209991  339.989990  336.018188   \n",
       "15  2019-08-22  342.429993  356.440002  341.010010  354.410004  350.269714   \n",
       "16  2019-08-23  355.350006  369.690002  354.350006  356.010010  351.851044   \n",
       "17  2019-08-26  365.040009  367.339996  356.890015  359.040009  354.845642   \n",
       "18  2019-08-27  359.420013  361.309998  353.619995  354.730011  350.585999   \n",
       "19  2019-08-28  353.309998  360.190002  351.179993  359.970001  355.764771   \n",
       "20  2019-08-29  363.850006  366.040009  360.170013  362.739990  358.502411   \n",
       "21  2019-08-30  364.369995  366.369995  362.399994  364.089996  359.836639   \n",
       "\n",
       "      Volume  \n",
       "0    6465300  \n",
       "1    4675400  \n",
       "2    5876100  \n",
       "3    3653300  \n",
       "4    4187600  \n",
       "5    3673000  \n",
       "6    3102900  \n",
       "7    2542500  \n",
       "8    3689700  \n",
       "9    5110400  \n",
       "10   4721800  \n",
       "11   3866600  \n",
       "12   2335600  \n",
       "13   2039000  \n",
       "14   3265600  \n",
       "15   8320900  \n",
       "16  11518700  \n",
       "17   4661100  \n",
       "18   3740300  \n",
       "19   2402000  \n",
       "20   2717500  \n",
       "21   3147000  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the real stock price for August 1st 2019 - August 31st 2019\n",
    "\n",
    "dataset_test = pd.read_csv('test_data.csv')\n",
    "dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_stock_price = dataset_test.iloc[:, 1:2].values\n",
    "real_stock_price.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[341.910004],\n",
       "       [334.559998],\n",
       "       [332.459991],\n",
       "       [333.98999 ],\n",
       "       [328.600006],\n",
       "       [331.299988],\n",
       "       [334.059998],\n",
       "       [333.679993],\n",
       "       [331.820007],\n",
       "       [328.350006],\n",
       "       [323.      ],\n",
       "       [330.220001],\n",
       "       [334.179993],\n",
       "       [332.470001],\n",
       "       [335.480011],\n",
       "       [342.429993],\n",
       "       [355.350006],\n",
       "       [365.040009],\n",
       "       [359.420013],\n",
       "       [353.309998],\n",
       "       [363.850006],\n",
       "       [364.369995]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_stock_price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict the stock price value for each day in August 2019, we need the values in the last 90 days.\n",
    "\n",
    "To obtain this **history** we need to combine both the training and test sets in one.\n",
    "\n",
    "If we were to use the training_set and test_set we would need to use the scaler  but that would change the actual test values.  Thus concatenate the original data frames!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predicted stock price of the future\n",
    "\n",
    "# Axis = 0 means concatenate the lines (for example: vertical axis)\n",
    "dataset_total = pd.concat((dataset_train['Open'], dataset_test['Open']), axis = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2559"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_total.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The contrast in the length of the first two gives us the first day in July 2019, and we need to go back 90 days to get the necessary range\n",
    "inputs = dataset_total[len(dataset_total) - len(dataset_test) - 90:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I did not use iloc from panda so lets rearrange the numpy array for compatibility, for example: all the values from input lines to be stacked in one column. The -1 means that the numpy has no knowledge of how the values were stored in lines. The 1 means I want to them in one column\n",
    "\n",
    "inputs = inputs.reshape(-1,1) \n",
    "\n",
    "# Applying the feature scaler\n",
    "inputs = sc.transform(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For each price in Aug. 2019 we need the **immediate 90 values** before it. \n",
    "2. We have 22 prices in August;\n",
    "3. We need a numpy 3D array of 90 prices (columns) times 22 days (rows) times 1 dependent variable \n",
    "4. We don’t need y_test. That is what we are trying to compute!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predicted stock price of the future\n",
    "X_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first 90 from inputs are from training set, starting from 90 and getting the extra 22\n",
    "for i in range(90, 112): \n",
    "    X_test.append(inputs[i-90:i, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a 3D structure\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_stock_price = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I need to inverse the scaling to get useful predicted stock price\n",
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price) \n",
    "predicted_stock_price.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualisation of the results by graph\n",
    "\n",
    "plt.plot(real_stock_price, color = 'red', label = 'Real Boeing Stock Price')\n",
    "\n",
    "plt.plot(predicted_stock_price, color = 'blue', label = 'Predicted Boeing Stock Price')\n",
    "\n",
    "plt.title('Boeing Stock Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Boeing Stock Price')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blue line shows the trend of the stock for the month of August 2019. \n",
    "\n",
    "Some observations:\n",
    "- The prediction lags behind the actual price curve because the model cannot react to fast non-linear changes. Spikes are examples of fast non-linear changes\n",
    "- Model reacts pretty well to smooth changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the RMSE\n",
    "\n",
    "If we need to compute the RMSE for our Stock Price Prediction problem, we use the real stock price and predicted stock price as shown.\n",
    "\n",
    "Then consider dividing this RMSE by the range of the Boeing Stock Price values of August 2019 to get a relative error, as opposed to an absolute error. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.4791523207726"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse = math.sqrt( mean_squared_error( real_stock_price[0:30,:], predicted_stock_price))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new data need to be placed in the same order/format  as in the case of the training/test sets.\n",
    "\n",
    "1. Getting more training data: we trained our model on the past 10 years of the  Boeing Stock Price.\n",
    "\n",
    "2. Increasing the number of time steps: the model remembered the stock price from the 90 previous financial days to predict the stock price of the next day. That’s because we chose a number of 90 time steps.\n",
    "\n",
    "3. Adding some other indicators: if you have the financial instinct that the stock price of some other companies might be correlated to the one of Boeing, you could add this other stock price as a new indicator in the training data.\n",
    "\n",
    "4. Adding more LSTM layers: we built a RNN with four LSTM layers but you could try with even more.\n",
    "\n",
    "5. Adding more neurons in the LSTM layers: we highlighted the fact that we needed a high number of neurons in the LSTM layers to respond better to the complexity of the problem and we chose to include 50 neurons in each of our 4 LSTM layers. You could try an architecture with even more neurons in each of the 4 (or more) LSTM layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning the RNN\n",
    "\n",
    "Parameter Tuning on the RNN model: we are dealing with a Regression problem because we predict a continuous outcome.\n",
    "\n",
    "**Tip**: replace: scoring = 'accuracy' by scoring = 'neg_mean_squared_error' in the GridSearchCV class parameters as we did in the ANN case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
